{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Labor Force Survey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [OpenDP documentation](https://docs.opendp.org) uses data derived from the [EU Labor Force Survey](https://ec.europa.eu/eurostat/web/microdata/public-microdata/labour-force-survey):\n",
    "\n",
    "> Public microdata, also referred to as public use files, for the EU Labour force survey (LFS) were created to enable interested parties to become familiar with microdata.\n",
    ">\n",
    "> At the same time, the privacy of respondents had to be protected. The structure of public microdata is the same as that of research microdata available in scientific use files.\n",
    ">\n",
    "> Public microdata enable researchers and trainers to develop programmes using the same formats and variable names as for the actual LFS scientific use files. The files have been designed so that programmes and procedures created with public microdata will also work with scientific use files.\n",
    "\n",
    "Code developed to work with a public microdata set like this could also be used with the scientific use files, and we believe that differential privacy would be a good tool to ensure that statistics derived from scientific use files could not inadvertantly reveal personal information.\n",
    "\n",
    "To reduce the download size needed for the tutorial, we have preprocessed the data by selecting a single country (France), dropping unused columns, and sampling a random subset of the rows. The code we'll present in the tutorials could be run on the original public microdata, or for that matter, the full private scientific use files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FR_PUF_LFS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m \n\u001b[1;32m      5\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFR_PUF_LFS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Y\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m filename:\n\u001b[1;32m      8\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFR_PUF_LFS\u001b[39m\u001b[38;5;124m'\u001b[39m, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FR_PUF_LFS'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import polars as pl \n",
    "\n",
    "dfs = []\n",
    "for filename in os.listdir('FR_PUF_LFS'):\n",
    "    if filename.endswith('.csv') and \"_Y\" not in filename:\n",
    "        file_path = os.path.join('FR_PUF_LFS', filename)\n",
    "        temp = pd.read_csv(file_path)\n",
    "        dfs.append(temp)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "#drop all columns and rows with only nan values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df = df.dropna(axis=0, how='all')\n",
    "df['QUARTER'] = df['QUARTER'].apply(lambda x: int(x[-1]))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_FR_LFS.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(50_000, random_state=1)\n",
    "sample.to_csv('sample_FR_LFS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
